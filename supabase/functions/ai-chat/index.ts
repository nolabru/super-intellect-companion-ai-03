import { serve } from "https://deno.land/std@0.168.0/http/server.ts";
import { corsHeaders, handleCors } from "./utils/cors.ts";
import { logError } from "./utils/logging.ts";
import { checkUserTokens } from "./utils/tokenManager.ts";

// Import model services
import * as lumaService from "./services/models/luma.ts";
import * as openaiService from "./services/models/openai.ts";
import * as anthropicService from "./services/models/anthropic.ts";
import * as elevenlabsService from "./services/models/elevenlabs.ts";
import * as geminiService from "./services/models/gemini.ts";
import * as deepseekService from "./services/models/deepseek.ts";
import * as kliginService from "./services/models/kligin.ts";

// Import orchestrator service
import * as orchestratorService from "./services/orchestrator.ts";

// Define response type
interface ResponseData {
  content: string;
  files?: string[];
  tokenInfo?: {
    tokensUsed: number;
    tokensRemaining: number;
  };
  modeSwitch?: {
    newMode: string;
    newModel: string;
  };
  googleIntegrationResults?: any[];
}

// Token mocado para testes com a Luma API - usando o token fornecido pelo usu√°rio
const MOCKED_LUMA_TOKEN = "luma-d0412b33-742d-4c23-bea2-cf7a8e2af184-ef7762ab-c1c6-4e73-b6d4-42078e8c7775";
// Token mocado para testes com a Kligin API
const MOCKED_KLIGIN_TOKEN = Deno.env.get("KLIGIN_API_KEY") || "";

// Main handler for all AI chat requests
async function handleAIChat(req: Request): Promise<Response> {
  try {
    // Extrair dados da requisi√ß√£o
    const { 
      content, 
      mode, 
      modelId, 
      files, 
      params, 
      userId, 
      conversationId,
      conversationHistory 
    } = await req.json();
    
    console.log(`[AI-Chat] Recebida solicita√ß√£o para modelo ${modelId} no modo ${mode}`, {
      contentLength: content?.length,
      filesCount: files?.length,
      paramsPreview: params ? JSON.stringify(params).substring(0, 100) : 'none',
      userIdProvided: !!userId,
      conversationIdProvided: !!conversationId
    });
    
    // Vari√°veis que podem ser alteradas pelo orquestrador
    let processedContent = content;
    let processedMode = mode;
    let processedModelId = modelId;
    let modeSwitchDetected = false;
    let googleIntegrationResults: any[] | undefined;
    
    // Aplicar orquestrador se userId estiver presente e n√£o for solicita√ß√£o de m√≠dia direta
    if (userId && mode === "text") {
      try {
        console.log(`[AI-Chat] Iniciando orquestrador para analisar a mensagem`);
        
        // Processar a mensagem com o orquestrador
        const orchestratorResult = await orchestratorService.processUserMessage(
          content,
          userId,
          mode,
          modelId,
          conversationHistory
        );
        
        // Verificar se o orquestrador sugeriu troca de modo
        if (orchestratorResult.detectedMode !== mode) {
          console.log(`[AI-Chat] Orquestrador detectou troca de modo: ${mode} -> ${orchestratorResult.detectedMode}`);
          processedMode = orchestratorResult.detectedMode;
          modeSwitchDetected = true;
        }
        
        // Verificar se o orquestrador sugeriu troca de modelo
        if (orchestratorResult.recommendedModel !== modelId) {
          console.log(`[AI-Chat] Orquestrador recomendou troca de modelo: ${modelId} -> ${orchestratorResult.recommendedModel}`);
          processedModelId = orchestratorResult.recommendedModel;
        }
        
        // Usar prompt melhorado do orquestrador
        processedContent = orchestratorResult.enhancedPrompt;
        console.log(`[AI-Chat] Prompt melhorado pelo orquestrador: "${processedContent.substring(0, 50)}..."`);
        
        // Se existirem itens de mem√≥ria extra√≠dos, salv√°-los
        if (orchestratorResult.memoryExtracted && orchestratorResult.memoryItems) {
          // Aqui seria o c√≥digo para salvar os itens de mem√≥ria
          console.log(`[AI-Chat] Itens de mem√≥ria extra√≠dos pelo orquestrador`);
        }
        
        // Enriquecer o prompt com contexto de mem√≥ria (se dispon√≠vel)
        if (userId) {
          const memoryContext = await orchestratorService.getUserMemoryContext(userId);
          if (memoryContext) {
            processedContent = orchestratorService.enrichPromptWithMemory(processedContent, memoryContext);
            console.log(`[AI-Chat] Prompt enriquecido com contexto de mem√≥ria`);
          }
        }
        
        // Processar a√ß√µes de integra√ß√£o com o Google, se houver
        if (orchestratorResult.googleIntegrationActions && orchestratorResult.googleIntegrationActions.length > 0) {
          console.log(`[AI-Chat] Orquestrador detectou ${orchestratorResult.googleIntegrationActions.length} a√ß√µes para Google`);
          
          const googleResult = await orchestratorService.processGoogleIntegrationActions(
            userId, 
            orchestratorResult.googleIntegrationActions
          );
          
          if (googleResult.success) {
            googleIntegrationResults = googleResult.results;
            
            // Adicionar informa√ß√µes sobre as a√ß√µes do Google ao prompt
            let googleContext = "\n\n**A√ß√µes realizadas nos servi√ßos Google:**\n";
            
            googleResult.results.forEach(result => {
              if (result.success) {
                // Adicionar informa√ß√µes espec√≠ficas com base no tipo de a√ß√£o
                if (result.actionType === 'calendar' && result.actionName === 'createEvent') {
                  googleContext += `‚úÖ Evento criado no Google Calendar: "${result.result.data.summary}"\n`;
                  googleContext += `üìÖ Link: ${result.result.eventLink}\n\n`;
                } 
                else if (result.actionType === 'drive' && result.actionName === 'createDocument') {
                  googleContext += `‚úÖ Documento criado no Google Drive: "${result.result.data.name}"\n`;
                  googleContext += `üìÑ Link: ${result.result.documentLink}\n\n`;
                }
                else if (result.actionType === 'sheets' && result.actionName === 'createSpreadsheet') {
                  googleContext += `‚úÖ Planilha criada no Google Sheets: "${result.result.data.properties.title}"\n`;
                  googleContext += `üìä Link: ${result.result.spreadsheetLink}\n\n`;
                }
                else if (result.actionType === 'calendar' && result.actionName === 'listEvents') {
                  googleContext += `‚úÖ Consultei sua agenda no Google Calendar\n`;
                  const events = result.result.events || [];
                  if (events.length > 0) {
                    googleContext += `üìÖ Pr√≥ximos eventos:\n`;
                    events.slice(0, 5).forEach((event: any) => {
                      const start = event.start.dateTime ? new Date(event.start.dateTime).toLocaleString('pt-BR') : event.start.date;
                      googleContext += `   - ${event.summary} (${start})\n`;
                    });
                    googleContext += "\n";
                  } else {
                    googleContext += `üìÖ N√£o encontrei eventos pr√≥ximos na sua agenda.\n\n`;
                  }
                }
                else {
                  googleContext += `‚úÖ ${result.description || `A√ß√£o ${result.actionName} conclu√≠da com sucesso`}\n\n`;
                }
              } else {
                googleContext += `‚ùå ${result.description || `A√ß√£o ${result.actionName}`} falhou: ${result.error}\n\n`;
              }
            });
            
            processedContent += googleContext;
            console.log(`[AI-Chat] Prompt enriquecido com resultados das a√ß√µes do Google`);
          } else {
            console.log(`[AI-Chat] Falha ao processar a√ß√µes do Google: ${JSON.stringify(googleResult)}`);
          }
        }
        
      } catch (orchestratorError) {
        console.error(`[AI-Chat] Erro no orquestrador, continuando com dados originais:`, orchestratorError);
        // Em caso de erro, continuamos com o prompt original
      }
    }
    
    // First check if user has enough tokens for this operation
    let tokensUsed = 0;
    let tokensRemaining = 0;
    
    if (userId) {
      try {
        // Usar o modo e modelo processados pelo orquestrador
        const tokenCheck = await checkUserTokens(userId, processedModelId, processedMode);
        
        if (!tokenCheck.hasEnoughTokens) {
          console.log(`User ${userId} does not have enough tokens for this operation: ${tokenCheck.tokensRequired} required, ${tokenCheck.tokensRemaining} remaining`);
          return new Response(
            JSON.stringify({
              content: tokenCheck.error || "N√£o h√° tokens suficientes para essa opera√ß√£o",
              tokenInfo: {
                tokensRequired: tokenCheck.tokensRequired,
                tokensRemaining: tokenCheck.tokensRemaining || 0
              },
              error: "INSUFFICIENT_TOKENS"
            }),
            {
              headers: { ...corsHeaders, "Content-Type": "application/json" },
              status: 402 // Payment Required
            }
          );
        }
        
        tokensUsed = tokenCheck.tokensRequired || 0;
        tokensRemaining = tokenCheck.tokensRemaining || 0;
        console.log(`[AI-Chat] Token check passed for user ${userId}. Required: ${tokensUsed}, Remaining: ${tokenCheck.tokensRemaining}`);
      } catch (error) {
        // Log error but allow operation to proceed
        console.warn(`[AI-Chat] Erro ao verificar tokens para usu√°rio ${userId}, prosseguindo sem verifica√ß√£o:`, error);
      }
    } else {
      console.log(`[AI-Chat] No user ID provided, proceeding without token check`);
    }
    
    let response: ResponseData = {
      content: "N√£o foi poss√≠vel processar sua solicita√ß√£o."
    };
    
    // Process based on model and mode
    try {
      // Verifica√ß√£o espec√≠fica para modelos Luma
      if (processedModelId.includes("luma")) {
        // Usando token mocado para Luma
        console.log("[AI-Chat] Modelo Luma selecionado, usando token mocado configurado diretamente no c√≥digo");
        // Pr√©-configuramos a vari√°vel LUMA_TOKEN_MOCK no service
        lumaService.setMockedToken(MOCKED_LUMA_TOKEN);
        
        // Valida√ß√£o opcional para ver se o token funciona
        const isValid = await lumaService.testApiKey(MOCKED_LUMA_TOKEN);
        if (!isValid) {
          console.warn("[AI-Chat] O token mocado da Luma pode n√£o estar funcionando corretamente");
        }
      } else if (processedModelId.includes("kligin")) {
        // Usando token configurado para Kligin
        console.log("[AI-Chat] Modelo Kligin selecionado, verificando configura√ß√£o do token");
        
        // Set the Kligin API credentials from env vars
        const kliginApiKey = Deno.env.get("KLIGIN_API_KEY") || "ed7299a2098a4b06a5cb31a50a96dec4";
        const kliginApiSecret = Deno.env.get("KLIGIN_API_SECRET") || "3dd57f873a1745c3a21f972a8024b456";
        
        if (kliginApiKey && kliginApiSecret) {
          kliginService.setApiCredentials(kliginApiKey, kliginApiSecret);
          console.log("[AI-Chat] Credenciais Kligin configuradas com sucesso");
          
          // Valida√ß√£o opcional para ver se as credenciais funcionam
          try {
            const isValid = await kliginService.testApiCredentials(kliginApiKey, kliginApiSecret);
            if (!isValid) {
              console.warn("[AI-Chat] As credenciais do Kligin podem n√£o estar funcionando corretamente");
            }
          } catch (err) {
            console.error("[AI-Chat] Erro ao testar credenciais do Kligin:", err);
          }
        } else {
          console.error("[AI-Chat] Credenciais do Kligin n√£o configuradas corretamente");
          throw new Error("Credenciais do Kligin n√£o configuradas. Verifique as vari√°veis de ambiente KLIGIN_API_KEY e KLIGIN_API_SECRET.");
        }
      }
      
      // Valida√ß√µes para diversos modelos
      if (processedModelId.includes("gpt") || processedModelId.includes("dall-e")) {
        try {
          openaiService.verifyApiKey();
        } catch (error) {
          return new Response(
            JSON.stringify({
              content: error.message,
              error: "OPENAI_API_KEY n√£o configurada",
            }),
            {
              headers: { ...corsHeaders, "Content-Type": "application/json" },
              status: 401,
            }
          );
        }
      } else if (processedModelId.includes("claude")) {
        try {
          anthropicService.verifyApiKey();
        } catch (error) {
          return new Response(
            JSON.stringify({
              content: error.message,
              error: "ANTHROPIC_API_KEY n√£o configurada",
            }),
            {
              headers: { ...corsHeaders, "Content-Type": "application/json" },
              status: 401,
            }
          );
        }
      } else if (processedModelId.includes("eleven") || processedModelId.includes("elevenlabs-tts")) {
        try {
          elevenlabsService.verifyApiKey();
        } catch (error) {
          return new Response(
            JSON.stringify({
              content: error.message,
              error: "ELEVENLABS_API_KEY n√£o configurada",
            }),
            {
              headers: { ...corsHeaders, "Content-Type": "application/json" },
              status: 401,
            }
          );
        }
      } else if (processedModelId.includes("gemini")) {
        try {
          geminiService.verifyApiKey();
        } catch (error) {
          return new Response(
            JSON.stringify({
              content: error.message,
              error: "GEMINI_API_KEY n√£o configurada",
            }),
            {
              headers: { ...corsHeaders, "Content-Type": "application/json" },
              status: 401,
            }
          );
        }
      } else if (processedModelId.includes("deepseek")) {
        try {
          deepseekService.verifyApiKey();
        } catch (error) {
          console.error("[AI-Chat] Erro na verifica√ß√£o do DEEPSEEK_API_KEY:", error);
          return new Response(
            JSON.stringify({
              content: error.message,
              error: "DEEPSEEK_API_KEY n√£o configurada",
            }),
            {
              headers: { ...corsHeaders, "Content-Type": "application/json" },
              status: 401,
            }
          );
        }
      } else if (processedModelId.includes("kligin")) {
        try {
          kliginService.verifyApiKey();
        } catch (error) {
          console.error("[AI-Chat] Erro na verifica√ß√£o do KLIGIN_API_KEY:", error);
          return new Response(
            JSON.stringify({
              content: error.message,
              error: "KLIGIN_API_KEY n√£o configurada",
            }),
            {
              headers: { ...corsHeaders, "Content-Type": "application/json" },
              status: 401,
            }
          );
        }
      }
      
      // Processar solicita√ß√£o com base no modo e modelo processados pelo orquestrador
      // Luma AI models
      if (processedModelId === "luma-video" && processedMode === "video") {
        console.log("[AI-Chat] Iniciando processamento de v√≠deo com Luma AI");
        const imageUrl = files && files.length > 0 ? files[0] : undefined;
        
        const videoParams = {
          ...params,
          model: params?.model || "ray-2"
        };
        
        const controller = new AbortController();
        const timeoutId = setTimeout(() => {
          console.log("[AI-Chat] Atingido tempo m√°ximo global para processamento de v√≠deo (5 minutos)");
          controller.abort();
        }, 300000); // 5 minutos
        
        try {
          response = await Promise.race([
            lumaService.generateVideo(processedContent, videoParams, imageUrl),
            new Promise<ResponseData>((_, reject) => {
              controller.signal.addEventListener('abort', () => {
                reject(new Error("Tempo limite global excedido para processamento de v√≠deo (5 minutos)"));
              });
            })
          ]);
          clearTimeout(timeoutId);
          console.log("[AI-Chat] Processamento de v√≠deo conclu√≠do com sucesso");
          
          if (response.files && response.files.length > 0) {
            response.files[0] = addTimestampToUrl(response.files[0]);
          }
        } catch (error) {
          clearTimeout(timeoutId);
          throw error;
        }
      } 
      else if (processedModelId === "luma-image" && processedMode === "image") {
        console.log("[AI-Chat] Iniciando processamento de imagem com Luma AI");
        
        const imageParams = {
          ...params,
          model: params?.model || "luma-1.1"
        };
        
        response = await lumaService.generateImage(processedContent, imageParams);
        console.log("[AI-Chat] Processamento de imagem conclu√≠do com sucesso");
        
        if (response.files && response.files.length > 0) {
          response.files[0] = addTimestampToUrl(response.files[0]);
        }
      }
      // Kligin models
      else if (processedModelId === "kligin-video" && processedMode === "video") {
        console.log("[AI-Chat] Iniciando processamento de v√≠deo com Kligin AI");
        const imageUrl = files && files.length > 0 ? files[0] : undefined;
        
        const videoParams = {
          ...params,
          model: params?.model || "kling-v1"
        };
        
        const controller = new AbortController();
        const timeoutId = setTimeout(() => {
          console.log("[AI-Chat] Atingido tempo m√°ximo global para processamento de v√≠deo (5 minutos)");
          controller.abort();
        }, 300000); // 5 minutos
        
        try {
          response = await Promise.race([
            kliginService.generateVideo(processedContent, videoParams, imageUrl),
            new Promise<ResponseData>((_, reject) => {
              controller.signal.addEventListener('abort', () => {
                reject(new Error("Tempo limite global excedido para processamento de v√≠deo (5 minutos)"));
              });
            })
          ]);
          clearTimeout(timeoutId);
          console.log("[AI-Chat] Processamento de v√≠deo Kligin conclu√≠do com sucesso");
          
          if (response.files && response.files.length > 0) {
            response.files[0] = addTimestampToUrl(response.files[0]);
          }
        } catch (error) {
          clearTimeout(timeoutId);
          throw error;
        }
      }
      else if (processedModelId === "kligin-image" && processedMode === "image") {
        console.log("[AI-Chat] Iniciando processamento de imagem com Kligin AI");
        
        const imageParams = {
          ...params,
          model: params?.model || "kling-v1"
        };
        
        response = await kliginService.generateImage(processedContent, imageParams);
        console.log("[AI-Chat] Processamento de imagem Kligin conclu√≠do com sucesso");
        
        if (response.files && response.files.length > 0) {
          response.files[0] = addTimestampToUrl(response.files[0]);
        }
      }
      // OpenAI models 
      else if (processedModelId.includes("gpt") && processedMode === "text") {
        console.log("[AI-Chat] Iniciando processamento de texto com OpenAI");
        
        try {
          const apiKey = openaiService.verifyApiKey();
          console.log("[AI-Chat] Chave API do OpenAI verificada com sucesso");
          
          if (apiKey.length > 8) {
            const maskedKey = apiKey.substring(0, 4) + "..." + apiKey.substring(apiKey.length - 4);
            console.log(`[AI-Chat] Usando chave API do OpenAI: ${maskedKey}`);
          }
          
          response = await openaiService.generateText(processedContent, processedModelId);
          console.log("[AI-Chat] Processamento de texto conclu√≠do com sucesso");
        } catch (openaiError) {
          console.error("[AI-Chat] Erro detalhado ao processar texto com OpenAI:", openaiError);
          throw openaiError;
        }
      }
      else if (processedModelId.includes("gpt") && processedMode === "image" && files && files.length > 0) {
        console.log("[AI-Chat] Iniciando processamento de an√°lise de imagem com OpenAI");
        const imageUrl = files[0];
        response = await openaiService.processImage(processedContent, imageUrl, processedModelId);
        console.log("[AI-Chat] An√°lise de imagem conclu√≠da com sucesso");
      }
      else if ((processedModelId.includes("gpt") || processedModelId === "dall-e-3") && processedMode === "image" && (!files || files.length === 0)) {
        console.log("[AI-Chat] Iniciando gera√ß√£o de imagem com DALL-E via OpenAI");
        response = await openaiService.generateImage(processedContent, processedModelId === "gpt-4o" ? "dall-e-3" : processedModelId);
        console.log("[AI-Chat] Gera√ß√£o de imagem conclu√≠da com sucesso");
        
        if (response.files && response.files.length > 0) {
          response.files[0] = addTimestampToUrl(response.files[0]);
        }
      }
      // Outros modelos
      else if (processedModelId.includes("claude") && processedMode === "text") {
        console.log("[AI-Chat] Iniciando processamento de texto com Anthropic");
        response = await anthropicService.generateText(processedContent, processedModelId);
        console.log("[AI-Chat] Processamento de texto conclu√≠do com sucesso");
      }
      else if (processedModelId.includes("claude") && processedMode === "image") {
        console.log("[AI-Chat] Iniciando processamento de imagem com Anthropic");
        const imageUrl = files && files.length > 0 ? files[0] : undefined;
        if (imageUrl) {
          response = await anthropicService.processImage(processedContent, imageUrl, processedModelId);
        } else {
          response = { content: "Erro: Necess√°rio fornecer uma imagem para an√°lise." };
        }
      }
      else if (processedModelId.includes("gemini") && processedMode === "text") {
        console.log("[AI-Chat] Iniciando processamento de texto com Google Gemini");
        response = await geminiService.generateText(processedContent, processedModelId);
        console.log("[AI-Chat] Processamento de texto conclu√≠do com sucesso");
      }
      else if (processedModelId.includes("gemini") && processedMode === "image" && files && files.length > 0) {
        console.log("[AI-Chat] Iniciando processamento de an√°lise de imagem com Google Gemini Vision");
        const imageUrl = files[0];
        response = await geminiService.processImage(processedContent, imageUrl, processedModelId);
        console.log("[AI-Chat] An√°lise de imagem conclu√≠da com sucesso");
      }
      else if ((processedModelId === "eleven-labs" || processedModelId === "elevenlabs-tts") && processedMode === "audio") {
        console.log("[AI-Chat] Iniciando gera√ß√£o de √°udio com ElevenLabs");
        const voiceParams = {
          voiceId: params?.voiceId || "EXAVITQu4vr4xnSDxMaL", // Sarah por padr√£o
          model: params?.model || "eleven_multilingual_v2",
          stability: params?.stability || 0.5,
          similarityBoost: params?.similarityBoost || 0.75,
          style: params?.style || 0,
          speakerBoost: params?.speakerBoost || true
        };
        
        response = await elevenlabsService.generateSpeech(processedContent, voiceParams);
        console.log("[AI-Chat] Gera√ß√£o de √°udio conclu√≠da com sucesso");
        
        if (response.files && response.files.length > 0) {
          response.files[0] = addTimestampToUrl(response.files[0]);
        }
      }
      else if ((processedModelId === 'deepseek-chat' || processedModelId === 'deepseek-coder') && processedMode === 'text') {
        console.log("[AI-Chat] Iniciando processamento de texto com Deepseek");
        response = await deepseekService.generateText(processedContent, processedModelId);
        console.log("[AI-Chat] Processamento de texto conclu√≠do com sucesso");
      }
      else {
        return new Response(
          JSON.stringify({
            content: `Combina√ß√£o de modelo e modo n√£o suportada: ${processedModelId} / ${processedMode}`,
            error: "Combina√ß√£o n√£o suportada",
          }),
          {
            headers: { ...corsHeaders, "Content-Type": "application/json" },
            status: 400,
          }
        );
      }
      
      console.log(`[AI-Chat] Resposta do modelo ${processedModelId} no modo ${processedMode} gerada com sucesso:`, {
        hasFiles: response.files && response.files.length > 0,
        fileCount: response.files?.length || 0,
        contentLength: response.content?.length || 0
      });
      
      // Adicionar os resultados das a√ß√µes do Google √† resposta, se houver
      if (googleIntegrationResults) {
        response.googleIntegrationResults = googleIntegrationResults;
      }
      
      // If user ID was provided, add token info to response
      if (userId) {
        response.tokenInfo = {
          tokensUsed: tokensUsed,
          tokensRemaining: tokensRemaining
        };
      }
      
      // Se houver uma troca de modo detectada, adicionar √† resposta
      if (modeSwitchDetected) {
        response.modeSwitch = {
          newMode: processedMode,
          newModel: processedModelId
        };
      }
      
    } catch (error) {
      const errorMessage = error instanceof Error ? error.message : String(error);
      console.error("[AI-Chat] Erro detalhado no processamento:", error);
      logError("SERVICE_ERROR", { error: errorMessage, model: processedModelId, mode: processedMode });
      
      let friendlyError = `Erro ao processar solicita√ß√£o: ${errorMessage}`;
      
      // Mensagens de erro espec√≠ficas baseadas no erro original
      if (processedModelId.includes("gpt") || processedModelId === "dall-e-3") {
        if (errorMessage.includes("API key") || errorMessage.includes("authorize") || errorMessage.includes("authenticate")) {
          friendlyError = "Erro de configura√ß√£o: A chave API do OpenAI n√£o est√° configurada corretamente. Por favor, verifique suas configura√ß√µes.";
        } else if (errorMessage.includes("billing") || errorMessage.includes("429") || errorMessage.includes("rate limit")) {
          friendlyError = "Erro de limite: Voc√™ excedeu seu limite de uso da OpenAI. Verifique seu plano e faturamento.";
        } else {
          friendlyError = `Erro na gera√ß√£o de ${processedMode === 'image' ? 'imagem' : 'texto'} com OpenAI: ${errorMessage}`;
        }
      } else if (processedModelId.includes("luma")) {
        if (errorMessage.includes("API key") || errorMessage.includes("Authorization") || errorMessage.includes("authenticate")) {
          friendlyError = "Erro de configura√ß√£o: A chave API do Luma AI n√£o est√° configurada corretamente. Por favor, verifique suas configura√ß√µes.";
        } else if (errorMessage.includes("404") || errorMessage.includes("Not Found")) {
          friendlyError = "Erro na API Luma: Endpoint n√£o encontrado. A API pode ter sido atualizada.";
        } else if (errorMessage.includes("Tempo limite")) {
          friendlyError = `O processamento do ${processedMode === 'video' ? 'v√≠deo' : 'imagem'} excedeu o tempo limite. A gera√ß√£o pode estar em andamento no servidor do Luma AI, verifique seu painel de controle.`;
        } else {
          friendlyError = `Erro na gera√ß√£o do ${processedMode === 'video' ? 'v√≠deo' : 'imagem'} com Luma AI: ${errorMessage}`;
        }
      } else if (processedModelId.includes("kligin")) {
        if (errorMessage.includes("API key") || errorMessage.includes("Authorization") || errorMessage.includes("authenticate")) {
          friendlyError = "Erro de configura√ß√£o: A chave API do Kligin AI n√£o est√° configurada corretamente. Por favor, verifique suas configura√ß√µes.";
        } else if (errorMessage.includes("404") || errorMessage.includes("Not Found")) {
          friendlyError = "Erro na API Kligin: Endpoint n√£o encontrado. A API pode ter sido atualizada.";
        } else if (errorMessage.includes("Tempo limite")) {
          friendlyError = `O processamento do ${processedMode === 'video' ? 'v√≠deo' : 'imagem'} excedeu o tempo limite. A gera√ß√£o pode estar em andamento no servidor do Kligin AI.`;
        } else {
          friendlyError = `Erro na gera√ß√£o do ${processedMode === 'video' ? 'v√≠deo' : 'imagem'} com Kligin AI: ${errorMessage}`;
        }
      } else if (processedModelId.includes("eleven")) {
        if (errorMessage.includes("API key") || errorMessage.includes("xi-api-key") || errorMessage.includes("authenticate")) {
          friendlyError = "Erro de configura√ß√£o: A chave API do ElevenLabs n√£o est√° configurada corretamente. Por favor, verifique suas configura√ß√µes.";
        } else {
          friendlyError = `Erro na gera√ß√£o de √°udio com ElevenLabs: ${errorMessage}`;
        }
      } else if (processedModelId.includes("gemini")) {
        if (errorMessage.includes("API key") || errorMessage.includes("authenticate")) {
          friendlyError = "Erro de configura√ß√£o: A chave API do Google Gemini n√£o est√° configurada corretamente. Por favor, verifique suas configura√ß√µes.";
        } else {
          friendlyError = `Erro na gera√ß√£o com Google Gemini: ${errorMessage}`;
        }
      } else if (processedModelId.includes("deepseek")) {
        if (errorMessage.includes("API key") || errorMessage.includes("authorize") || errorMessage.includes("authenticate")) {
          friendlyError = "Erro de configura√ß√£o: A chave API do Deepseek n√£o est√° configurada corretamente. Por favor, verifique suas configura√ß√µes.";
        } else {
          friendlyError = `Erro na gera√ß√£o de texto com Deepseek: ${errorMessage}`;
        }
      }
      
      return new Response(
        JSON.stringify({
          content: friendlyError,
          error: errorMessage,
        }),
        {
          headers: { ...corsHeaders, "Content-Type": "application/json" },
          status: 400,
        }
      );
    }
    
    return new Response(JSON.stringify(response), {
      headers: { ...corsHeaders, "Content-Type": "application/json" },
      status: 200,
    });
  } catch (error) {
    console.error("[AI-Chat] Erro ao processar solicita√ß√£o:", error);
    
    const errorMessage = error instanceof Error ? error.message : "Erro desconhecido";
    logError("REQUEST_ERROR", { error: errorMessage });
    
    return new Response(
      JSON.stringify({
        content: `Erro: ${errorMessage}`,
        error: errorMessage,
      }),
      {
        headers: { ...corsHeaders, "Content-Type": "application/json" },
        status: 500,
      }
    );
  }
}

// Helper function to add timestamp to URLs for cache busting
function addTimestampToUrl(url: string): string {
  if (!url) return url;
  
  // Don't add timestamp to data URLs
  if (url.startsWith('data:')) return url;
  
  const separator = url.includes('?') ? '&' : '?';
  return `${url}${separator}t=${Date.now()}`;
}

// Setup the server
serve(async (req) => {
  // Handle CORS preflight requests
  const corsResponse = handleCors(req);
  if (corsResponse) {
    return corsResponse;
  }
  
  // Handle API request
  return handleAIChat(req);
});
